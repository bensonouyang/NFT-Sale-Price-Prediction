{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b2719e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Read data\n",
    "\n",
    "XYtr = pd.read_csv('/Users/tanlu/Desktop/~/2021 Fall/Stat 440/project2/stat440-21-project2/XYtr.csv')\n",
    "\n",
    "\n",
    "XYtr1 = XYtr.copy() # from stackoverflow\n",
    "XYtr1['cdate'] = pd.to_datetime(XYtr1['cdate']).values.astype(np.float64)/8.64e+13\n",
    "\n",
    "Ytr1 = XYtr1['total'].copy()\n",
    "Xtr1 = XYtr1[['X.sales', 'cdate', 'fee1', 'fee2']].copy()\n",
    "\n",
    "Xtr1 = Xtr1.astype(np.float64)\n",
    "\n",
    "Xtr1 = Xtr1.fillna(0)\n",
    "\n",
    "pred = pd.read_csv('/Users/tanlu/Desktop/~/2021 Fall/Stat 440/project2/stat440-21-project2/pred.csv')\n",
    "\n",
    "Xte = pd.read_csv('/Users/tanlu/Desktop/~/2021 Fall/Stat 440/project2/stat440-21-project2/Xte.csv')\n",
    "\n",
    "Xte1 = Xte.copy()\n",
    "Xte1['cdate'] = pd.to_datetime(Xte1['cdate']).values.astype(np.float64)/8.64e+13\n",
    "Xte1 = Xte1[['X.sales', 'cdate', 'fee1', 'fee2']].copy()\n",
    "Xte1 = Xte1.astype(np.float64)\n",
    "Xte1 = Xte1.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2013d947",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "model3 = SGDRegressor(loss = 'epsilon_insensitive', alpha = 0, epsilon = 0).fit(Xtr1, Ytr1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0c5203cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/Xte_fi.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/zk/fnlmhq6s03z80vqlhybv8d940000gn/T/ipykernel_44202/1036298120.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/Xte_fi.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'id,fi1,fi2,fi3,fi4,fi5,fi6,fi7\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXte\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/Xte_fi.csv'"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import imageio\n",
    "import numpy as np\n",
    "\n",
    "fp = open('/Users/tanlu/Desktop/~/2021 Fall/Stat 440/project2/stat440-21-project2/Xtr_fi.csv', 'w')\n",
    "fp.write('id,fi1,fi2,fi3,fi4,fi5,fi6,fi7\\n')\n",
    "for i in range(XYtr.shape[0]):\n",
    "    id = XYtr.loc[i,'id']\n",
    "    f = XYtr.loc[i,'id'] + XYtr.loc[i, 'ext']\n",
    "    try:\n",
    "        pic = imageio.imread('/Users/tanlu/Desktop/~/2021 Fall/Stat 440/project2/stat440-21-project2/images/images/' + f)\n",
    "        fi1 = pic.shape[0] # width\n",
    "        fi2 = pic.shape[1] # height\n",
    "        fi3 = np.mean(pic[:,:,:]) # mean intensity of the image, brightness\n",
    "        fi4 = pic.min() # minimum is the darkest portion of the image\n",
    "        fi5 = np.mean(pic[:,:,0]) # \n",
    "        fi6 = np.mean(pic[:,:,1])\n",
    "        fi7 = np.mean(pic[:,:,2])\n",
    "    \n",
    "    except:\n",
    "        fi1 = np.nan\n",
    "        fi2 = np.nan\n",
    "        fi3 = np.nan\n",
    "        fi4 = np.nan\n",
    "        fi5 = np.nan\n",
    "        fi6 = np.nan\n",
    "        fi7 = np.nan\n",
    "        pass\n",
    "    \n",
    "    fp.write('%s,%f,%f,%f,%f,%f,%f,%f\\n' % (id, fi1,fi2,fi3,fi4,fi5,fi6,fi7))\n",
    "\n",
    "fp.close()\n",
    "\n",
    "fp = open('data/Xte_fi.csv', 'w')\n",
    "fp.write('id,fi1,fi2,fi3,fi4,fi5,fi6,fi7\\n')\n",
    "for i in range(Xte.shape[0]):\n",
    "    id = XYtr.loc[i,'id']\n",
    "    f = XYtr.loc[i,'id'] + XYtr.loc[i, 'ext']\n",
    "    try:\n",
    "        pic = imageio.imread('/Users/tanlu/Desktop/~/2021 Fall/Stat 440/project2/stat440-21-project2/images/images/' + f)\n",
    "        fi1 = pic.shape[0] # width\n",
    "        fi2 = pic.shape[1] # height\n",
    "        fi3 = np.mean(pic[:,:,:]) # mean intensity of the image, brightness\n",
    "        fi4 = pic.min() # minimum is the darkest portion of the image\n",
    "        fi5 = np.mean(pic[:,:,0]) \n",
    "        fi6 = np.mean(pic[:,:,1])\n",
    "        fi7 = np.mean(pic[:,:,2])\n",
    "    \n",
    "    except:\n",
    "        fi1 = np.nan\n",
    "        fi2 = np.nan\n",
    "        fi3 = np.nan\n",
    "        fi4 = np.nan\n",
    "        fi5 = np.nan\n",
    "        fi6 = np.nan\n",
    "        fi7 = np.nan\n",
    "        pass\n",
    "    \n",
    "    fp.write('%s,%f,%f,%f,%f,%f,%f,%f\\n' % (id, fi1,fi2,fi3,fi4,fi5,fi6,fi7))\n",
    "\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c4652681",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Make corpus and vocab\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "K = 10\n",
    "XYtr['description'] = XYtr['description'].fillna(\"NAN\")\n",
    "Xte['description'] = Xte['description'].fillna(\"NAN\")\n",
    "corpus = list(XYtr['description'])+list(Xte['description'])\n",
    "vectorizer = CountVectorizer()\n",
    "bow = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# https://stackoverflow.com/questions/62777131/how-to-remove-frequent-infrequent-features-from-sklearn-countvectorizer \n",
    "# Code to remove top X% most frequent words (use this to remove stop words).\n",
    "\n",
    "total_features = len(vectorizer.vocabulary_)\n",
    "top_vect = CountVectorizer(max_features=int(total_features * 0.1))\n",
    "top_bow = top_vect.fit_transform(corpus)\n",
    "\n",
    "# Create a list of (term, frequency) tuples sorted by their frequency\n",
    "sum_words = bow.sum(axis=0) \n",
    "words_freq = [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]\n",
    "words_freq = sorted(words_freq, key = lambda x: x[1], reverse = True)\n",
    "\n",
    "# Keep only the terms in a list\n",
    "vocabulary, _ = zip(*words_freq[:int(total_features*0.9)])\n",
    "vocabulary = list(vocabulary)\n",
    "\n",
    "bottom_vect = CountVectorizer(vocabulary=vocabulary)\n",
    "corpus = bottom_vect.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e4976990",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.12627303, 0.00416746, 0.00416698, ..., 0.00416694, 0.72996326,\n",
       "        0.00416825],\n",
       "       [0.01428609, 0.01429223, 0.01428644, ..., 0.01428588, 0.87141051,\n",
       "        0.01428645],\n",
       "       [0.00357159, 0.0035716 , 0.00357219, ..., 0.00357145, 0.00357161,\n",
       "        0.00357159],\n",
       "       ...,\n",
       "       [0.00303045, 0.00303046, 0.97272454, ..., 0.00303032, 0.00303046,\n",
       "        0.00303044],\n",
       "       [0.27370022, 0.00322636, 0.00322725, ..., 0.00322658, 0.00322649,\n",
       "        0.00322632],\n",
       "       [0.00357159, 0.0035716 , 0.00357219, ..., 0.00357145, 0.00357161,\n",
       "        0.00357159]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda = LatentDirichletAllocation(n_components = K)\n",
    "lda.fit(corpus)\n",
    "topics = lda.transform(corpus)\n",
    "N = XYtr.shape[0]\n",
    "topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "3f013bbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.41272357e-02, 2.53794763e-02, 1.10785018e-02, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [4.14905026e-03, 0.00000000e+00, 9.49580740e-03, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        2.36435598e-01, 0.00000000e+00, 0.00000000e+00],\n",
       "       ...,\n",
       "       [1.68938608e-02, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        5.08972082e-03, 0.00000000e+00, 1.30880754e-04],\n",
       "       [3.77500455e-02, 2.74156440e-02, 1.05799455e-02, ...,\n",
       "        0.00000000e+00, 4.35445096e-02, 0.00000000e+00],\n",
       "       [2.57978627e-04, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        2.34265640e-01, 3.92715078e-06, 4.94409608e-05]])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "model = NMF(n_components=K, init='random', random_state=0)\n",
    "model.fit(corpus)\n",
    "topics = model.transform(corpus)\n",
    "topics  #https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "43e0a4eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.44514736,  0.2581813 ,  0.2390802 , ..., -0.10310143,\n",
       "         0.62393678,  0.12994178],\n",
       "       [ 0.31584444,  0.0292451 ,  0.0317513 , ...,  0.03485077,\n",
       "         0.44780215, -0.35347414],\n",
       "       [ 3.31738456, -3.94647718,  0.65413359, ...,  0.51851242,\n",
       "         0.1945726 ,  0.21135198],\n",
       "       ...,\n",
       "       [ 3.82348818, -4.31512705, -0.58256104, ..., -0.55218988,\n",
       "        -0.40816224, -0.20042469],\n",
       "       [ 2.59864214,  0.43791885, -0.67200569, ..., -0.08676778,\n",
       "         0.63878151, -0.16338286],\n",
       "       [ 3.31417367, -3.92997903,  0.64487783, ...,  0.50242888,\n",
       "         0.188201  ,  0.20401623]])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TruncSVD\n",
    "\n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "model = TruncatedSVD(n_components=K, n_iter=7, random_state=0)\n",
    "model.fit(corpus)\n",
    "topics = model.transform(corpus)\n",
    "topics  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "57f269b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00573256, -0.01903385,  0.01716227, ...,  0.00071926,\n",
       "        -0.00170657,  0.00036358],\n",
       "       [-0.00874385, -0.03601306,  0.01522739, ...,  0.0037761 ,\n",
       "         0.00077169, -0.00618773],\n",
       "       [-0.00355072,  0.00543401, -0.05221697, ...,  0.01887486,\n",
       "         0.00733082,  0.00163375],\n",
       "       ...,\n",
       "       [-0.00260077,  0.0138497 , -0.05800613, ..., -0.0106918 ,\n",
       "        -0.00721132, -0.00782638],\n",
       "       [-0.00274553, -0.00192695,  0.02068258, ..., -0.01231826,\n",
       "         0.00362545, -0.01358288],\n",
       "       [-0.00355052,  0.00541854, -0.05194489, ...,  0.01837211,\n",
       "         0.00709648,  0.0016014 ]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html\n",
    "# Kernel PCA(doesn't improve the score)\n",
    "from sklearn.decomposition import KernelPCA\n",
    "model = KernelPCA(n_components=K, kernel='poly')\n",
    "model.fit(corpus)\n",
    "topics = model.transform(corpus)\n",
    "topics  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "6d9f257e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = open('/Users/tanlu/Desktop/~/2021 Fall/Stat 440/project2/stat440-21-project2/XYtr_ft.csv', 'w')\n",
    "fp.write('id')\n",
    "for k in range(K):\n",
    "    fp.write(',FT%04d' % k)\n",
    "\n",
    "fp.write('\\n')\n",
    "for i in range(N):\n",
    "    id = XYtr.loc[i,'id']\n",
    "    fp.write('%s' % id)\n",
    "    for k in range(K):\n",
    "        fp.write(',%f' % topics[i, k])\n",
    "    \n",
    "    fp.write('\\n')\n",
    "\n",
    "fp.close()    \n",
    "\n",
    "fp = open('/Users/tanlu/Desktop/~/2021 Fall/Stat 440/project2/stat440-21-project2/Xte_ft.csv', 'w')\n",
    "fp.write('id')\n",
    "for k in range(K):\n",
    "    fp.write(',FT%04d' % k)\n",
    "\n",
    "fp.write('\\n')\n",
    "for i in range(N):\n",
    "    id = Xte.loc[i,'id']\n",
    "    fp.write('%s' % id)\n",
    "    for k in range(K):\n",
    "        fp.write(',%f' % topics[i + N, k])\n",
    "    \n",
    "    fp.write('\\n')\n",
    "\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "c1730ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr_fi = pd.read_csv('/Users/tanlu/Desktop/~/2021 Fall/Stat 440/project2/stat440-21-project2/Xtr_fi.csv')\n",
    "Xtr_fi1 = Xtr_fi.copy()\n",
    "Xtr_fi1.drop('id',1, inplace = True)\n",
    "Xtr_fi1 = Xtr_fi1.fillna(0)\n",
    "\n",
    "Xte_fi = pd.read_csv('/Users/tanlu/Desktop/~/2021 Fall/Stat 440/project2/stat440-21-project2/Xte_fi.csv')\n",
    "Xte_fi1 = Xte_fi.copy()\n",
    "Xte_fi1.drop('id',1, inplace = True)\n",
    "Xte_fi1 = Xte_fi1.fillna(0)\n",
    "\n",
    "Xtr_ft = pd.read_csv('/Users/tanlu/Desktop/~/2021 Fall/Stat 440/project2/stat440-21-project2/XYtr_ft.csv')\n",
    "Xtr_ft1 = Xtr_ft.copy()\n",
    "Xtr_ft1.drop('id',1, inplace = True)\n",
    "Xtr_ft1 = Xtr_ft1.fillna(0)\n",
    "\n",
    "Xte_ft = pd.read_csv('/Users/tanlu/Desktop/~/2021 Fall/Stat 440/project2/stat440-21-project2/Xte_ft.csv')\n",
    "Xte_ft1 = Xte_ft.copy()\n",
    "Xte_ft1.drop('id',1, inplace = True)\n",
    "Xte_ft1 = Xte_ft1.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "02100b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr5 = pd.concat([Xtr1, Xtr_fi1, Xtr_ft1], axis = 1)\n",
    "Xte5 = pd.concat([Xte1, Xte_fi1, Xte_ft1], axis = 1)\n",
    "Xte5 = Xte5.astype(np.float64)\n",
    "Xte5 = Xte5.fillna(0)\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "Xtr5 = scaler.fit_transform(Xtr5)\n",
    "Xte5 = scaler.transform(Xte5)\n",
    "\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "model5 = SGDRegressor(loss = 'epsilon_insensitive', alpha = 0, epsilon = 0).fit(Xtr5, Ytr1)\n",
    "pred5 = pred.copy()\n",
    "pred5['total'] = model5.predict(Xte5)\n",
    "pred5.to_csv('/Users/tanlu/Desktop/~/2021 Fall/Stat 440/project2/stat440-21-project2/pred5.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6399ddff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79b9953",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabb9904",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b57f5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399d363e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
